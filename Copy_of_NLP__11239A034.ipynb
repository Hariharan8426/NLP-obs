{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hariharan8426/NLP-obs/blob/main/Copy_of_NLP__11239A034.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMATIZATION**"
      ],
      "metadata": {
        "id": "pqZiJES_RLsZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CGyzI5AQGxI",
        "outputId": "700298cc-4b08-4859-8b66-06990f0a140c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats → cat\n",
            "running → running\n",
            "better → better\n",
            "studies → study\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"cats\", \"running\", \"better\", \"studies\"]\n",
        "for w in words:\n",
        "    print(w, \"→\", lemmatizer.lemmatize(w))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NORMALIZATION**"
      ],
      "metadata": {
        "id": "ounJnUylRYJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"This is an Example: Normalizing Text in NLP!!!\"\n",
        "text = text.lower()                     # lowercase\n",
        "text = re.sub(r'[^\\w\\s]', '', text)     # remove punctuation\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcoKEMogQZ8a",
        "outputId": "70e6e4c4-1e5a-47d8-87e7-cb90a3d3d842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is an example normalizing text in nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TOKENIZATION**"
      ],
      "metadata": {
        "id": "zQnv1aM0Re2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"I love learning NLP with ChatGPT!\"\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7q19QVCQn9l",
        "outputId": "344c7166-3c63-4ca8-de87-d58fc5bde887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'learning', 'NLP', 'with', 'ChatGPT', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING**"
      ],
      "metadata": {
        "id": "-LPuDvqHRnXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"flies\", \"easily\", \"fairly\"]\n",
        "for w in words:\n",
        "    print(w, \"→\", stemmer.stem(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq4FM5sBQ5-c",
        "outputId": "f60eb050-6bd1-494d-c909-26d06341526b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running → run\n",
            "flies → fli\n",
            "easily → easili\n",
            "fairly → fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MORPHOLOGY**"
      ],
      "metadata": {
        "id": "VXxJAg64Rs9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"The cats are running quickly\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.lemma_, \"|\", token.pos_, \"|\", token.morph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mnTLA7QRxcN",
        "outputId": "97eb90be-4fc3-4da2-e477-a97ec0bcdbfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → the | DET | Definite=Def|PronType=Art\n",
            "cats → cat | NOUN | Number=Plur\n",
            "are → be | AUX | Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "running → run | VERB | Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "quickly → quickly | ADV | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SPELLING **CORRECTION**"
      ],
      "metadata": {
        "id": "tHFF4brtR3Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob -q\n",
        "from textblob import TextBlob\n",
        "text = \"I lik to lern naturall langauge procesing\"\n",
        "blob = TextBlob(text)\n",
        "print(blob.correct())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR3b7t_ZR-H3",
        "outputId": "d3a5437e-f555-43cc-f080-f7c360c42be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like to learn natural language processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DEDUCTION**"
      ],
      "metadata": {
        "id": "XrGUyU05SJeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "read_expr = Expression.fromstring\n",
        "\n",
        "kb = [\n",
        "    read_expr('man(Socrates)'),\n",
        "    read_expr('all x (man(x) -> mortal(x))')\n",
        "]\n",
        "goal = read_expr('mortal(Socrates)')\n",
        "print(ResolutionProver().prove(goal, kb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8MKE_FSSQhF",
        "outputId": "73e02e5e-1c6b-44a7-c481-ea6ffe8c2e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNIGRAM**"
      ],
      "metadata": {
        "id": "xy4gVDVZSXk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "freq = {}\n",
        "for word in words:\n",
        "    freq[word] = freq.get(word, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FO9ZXKpScsi",
        "outputId": "20312d26-0ae9-4146-c8bb-047146114e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': 2, 'love': 2, 'natural': 1, 'language': 1, 'processing': 1, 'and': 1, 'coding': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BIGRAM**"
      ],
      "metadata": {
        "id": "w1S_6z9TS0m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "bigrams = []\n",
        "for i in range(len(words) - 1):\n",
        "    bigrams.append((words[i], words[i+1]))\n",
        "\n",
        "freq = {}\n",
        "for bigram in bigrams:\n",
        "    freq[bigram] = freq.get(bigram, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxx29eKbS3Tn",
        "outputId": "4f77fae4-d7e2-46f1-c45c-71d6f2529a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('I', 'love'): 2, ('love', 'natural'): 1, ('natural', 'language'): 1, ('language', 'processing'): 1, ('processing', 'and'): 1, ('and', 'I'): 1, ('love', 'coding'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRIGRAM**"
      ],
      "metadata": {
        "id": "Etdd7pNhS-7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love natural language processing and I love coding\"\n",
        "\n",
        "words = text.split()\n",
        "\n",
        "trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    trigrams.append((words[i], words[i+1], words[i+2]))\n",
        "\n",
        "freq = {}\n",
        "for trigram in trigrams:\n",
        "    freq[trigram] = freq.get(trigram, 0) + 1\n",
        "\n",
        "print(freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHfuIjtTBci",
        "outputId": "181ffc75-6eda-4cc0-d534-b49febef478b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('I', 'love', 'natural'): 1, ('love', 'natural', 'language'): 1, ('natural', 'language', 'processing'): 1, ('language', 'processing', 'and'): 1, ('processing', 'and', 'I'): 1, ('and', 'I', 'love'): 1, ('I', 'love', 'coding'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-GRAM **SMOOTHING**"
      ],
      "metadata": {
        "id": "8Kq_wCJYTKvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "text = \"I love NLP I love machine learning\"\n",
        "words = text.split()\n",
        "V = len(set(words))  # Vocabulary size\n",
        "unigrams = Counter(words)\n",
        "bigrams = Counter([(words[i], words[i+1]) for i in range(len(words)-1)])\n",
        "def laplace_prob(w1, w2):\n",
        "    return (bigrams[(w1, w2)] + 1) / (unigrams[w1] + V)\n",
        "print(\"P(love | I) =\", laplace_prob(\"I\", \"love\"))\n",
        "print(\"P(NLP | love) =\", laplace_prob(\"love\", \"NLP\"))\n",
        "print(\"P(machine | NLP) =\", laplace_prob(\"NLP\", \"machine\"))\n",
        "print(\"P(learning | machine) =\", laplace_prob(\"machine\", \"learning\"))\n",
        "print(\"P(unknown | NLP) =\", laplace_prob(\"NLP\", \"unknown\"))  # unseen word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hmxyt2-NTTit",
        "outputId": "f6850b0d-3b14-4d61-a686-cd559345f411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(love | I) = 0.42857142857142855\n",
            "P(NLP | love) = 0.2857142857142857\n",
            "P(machine | NLP) = 0.16666666666666666\n",
            "P(learning | machine) = 0.3333333333333333\n",
            "P(unknown | NLP) = 0.16666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS **TAGGING**"
      ],
      "metadata": {
        "id": "Tk0xSUxETbf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "text = \"I love learning NLP\"\n",
        "words = nltk.word_tokenize(text)\n",
        "print(nltk.pos_tag(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_U8tCpVThFu",
        "outputId": "f3576d01-30c7-431b-f4eb-a42698a3f2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('love', 'VBP'), ('learning', 'VBG'), ('NLP', 'NNP')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HMM** **TAGGING**"
      ],
      "metadata": {
        "id": "el3FeSVdT35Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "train_data = [[\n",
        "    ('I', 'PRON'),\n",
        "    ('love', 'VERB'),\n",
        "    ('dogs', 'NOUN')\n",
        "], [\n",
        "    ('You', 'PRON'),\n",
        "    ('love', 'VERB'),\n",
        "    ('cats', 'NOUN')\n",
        "]]\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "tagger = trainer.train_supervised(train_data)\n",
        "sentence = ['I', 'love', 'cats']\n",
        "print(tagger.tag(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_u7egmlT6rW",
        "outputId": "8e0f4dca-1603-47d7-e420-d3dc0fd2985d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRON'), ('love', 'VERB'), ('cats', 'NOUN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BRILL POS **TAGGER **"
      ],
      "metadata": {
        "id": "ZFsHbqBaUCCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import brill, brill_trainer, UnigramTagger\n",
        "\n",
        "nltk.download('treebank', quiet=True)\n",
        "nltk.download('universal_tagset', quiet=True)\n",
        "data = nltk.corpus.treebank.tagged_sents(tagset='universal')[:3000]\n",
        "uni = UnigramTagger(data)\n",
        "tagger = brill_trainer.BrillTaggerTrainer(uni, brill.fntbl37()).train(data)\n",
        "print(tagger.tag(\"I love learning NLP\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGtcSpi4UM-1",
        "outputId": "2f36d835-49db-4313-abee-71f6a3766e2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRON'), ('love', None), ('learning', 'NOUN'), ('NLP', None)]\n"
          ]
        }
      ]
    }
  ]
}